---
title: ecs-service-with-alb Service Migration Guide
categories: Upgrades
image: /assets/img/guides/refresh_icon.png
excerpt: Learn how to update your v1 Reference Architecture to use the Gruntwork Service Catalog.
tags: ["aws", "terraform", "terragrunt"]
cloud: ["aws"]
redirect_from: /static/guides/upgrades/how-to-update-your-ref-arch/
---
:page-type: guide
:page-layout: post

:toc:
:toc-placement!:

// GitHub specific settings. See https://gist.github.com/dcode/0cfbf2699a1fe9b46ff04c41721dda74 for details.
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
toc::[]
endif::[]

== ecs-service-with-alb Service Migration Guide

Follow this guide to update the `ecs-service-with-alb` module to the Service Catalog.

=== Estimated Time to Migrate: 1 hour per environment

=== container_definitions Input

Configure the new `container_definitions` input variable.

In the original Reference Architecture, the `ecs-service-with-alb` module constructed the container definition with
default settings and only exposed a limited number of options as Terraform variables. However, in the Service Catalog,
the `ecs-service` module offers greater flexibility by directly exposing the container definitions as an input variable
(`container_definitions`).

Set these new configuration options that are required on the `container_definitions` input variable in the Service
Catalog version of the module:

* `name` (A reference to bind to the container)
* `logConfiguration` (Any configuration for centralized log aggregation)

Also, update the configuration for these inputs, which have been combined into the `container_definitions` input
variable in the Service Catalog version of the module:

* `image`
* `image_version`
* `cpu`
* `memory`
* `container_port`
* `**_env_var_name` and `**_remote_state_path`

For example, if you had the following configuration:

[source,python]
----
# BEFORE
inputs = {
  image          = "<ECS_CONTAINER_IMAGE_URL>"
  image_version  = "v1"
  cpu            = 512
  memory         = 128
  container_port = 3000

  db_remote_state_path = "data-stores/postgres/terraform.tfstate"
  db_url_env_var_name  = "DB_URL"

  redis_remote_state_path = "data-stores/redis/terraform.tfstate"
  redis_url_env_var_name  = "REDIS_URL"

  vpc_env_var_name  = "VPC_NAME"
}
----

Change the inputs to:

[source,python]
----
# AFTER
inputs = {
  container_definitions = [{
    image  = "<ECS_CONTAINER_IMAGE_URL>:v1"
    cpu    = 512
    memory = 128

    portMappings = [{
      containerPort = 3000
      protocol      = "tcp"
    }]

    # Environment variables that should be configured for your app.
    environment = [{
      name  = "DB_URL"
      # NOTE: this dependency should point to the RDS service config.
      value = dependency.postgres.outputs.primary_endpoint
    }, {
      name  = "REDIS_URL"
      # NOTE: this dependency should point to the Redis service config.
      value = dependency.redis.outputs.primary_endpoint
    }, {
      name  = "VPC_NAME"
      value = "dev"
    }]

    # The following variables are necessary to have a complete container definition
    # Name of the container.
    name = "myapp"
    # Configuration options for the container logs. Here we log to syslog on the EC2 instances.
    logConfiguration = {
      logDriver = "syslog"
      options = {
        # Note that the configuration options support docker formatting templates:
        # https://docs.docker.com/config/formatting/
        tag = "myapp ({{.ID}})"
      }
    }
  }]
}
----

=== elb_target_groups Input

As with `container_definitions`, you need to update the Target Group configurations for the ELB, which have been
consolidated to a new variable `elb_target_groups` in the Service Catalog.

Configure the `elb_target_groups` variable using the following template:

[source,python]
----
elb_target_groups = {
  alb = {
    name = "<SERVICE_NAME>"
    container_name = "<CONTAINER_NAME>" # This should be the same as the name attribute in container_definitions.
    container_port = 3000 # This should be the same as the portMappings attribute in container_definitions.
    protocol = "HTTPS" # NOTE: this was renamed from alb_target_group_protocol.
    health_check_protocol = "HTTPS" # NOTE: this was renamed from health_check_protocol.
  }
}
----

=== Listener Rules Input

Update your configuration of listener rules. Before, the listener rules were all configured internally in the module
using the `is_internal_alb` and `alb_listener_rule_configs` input variables. Now you need to configure them using the
`default_listener_arns`, `default_listener_ports`, and `forward_rules` input variables (using dependencies to look up
which ALB to bind the rules to).

For example, if you had the following config:

[source,python]
----
# BEFORE
inputs = {
  is_internal_alb = true
  alb_listener_rule_configs = [{
    port     = 443
    path     = "/refarch-demo-sample-app-backend*"
    priority = 100
  }]
}
----

Change the config to:

[source,python]
----
# AFTER
inputs = {
  # NOTE: this dependency should point to the internal-alb service config, and public alb if is_internal_alb was false.
  default_listener_arns  = dependency.internal_alb.outputs.listener_arns
  default_listener_ports = ["443"] # NOTE: this should be the same as the port in alb_listener_rule_configs.
  forward_rules = {
    main = {
      path     = "/refarch-demo-sample-app-backend*"
      priority = 100
    }
  }
}
----

=== New Required Inputs

Configure these new inputs to migrate to the Service Catalog version of the module. They are now required.

* `ecs_cluster_arn`: The ARN of the ECS cluster. This should be sourced using a `dependency` block against the
`ecs-cluster` service, using the `ecs_cluster_arn` output.
* `ecs_cluster_name`: The name of the ECS cluster. This should be sourced using a `dependency` block against the
`ecs-cluster` service, using the `ecs_cluster_name` output.
* `alarm_sns_topic_arns`: The ARNs of SNS topics for receiving alerts from CloudWatch. This should be pulled in with a
`dependency` block against the `sns-topic` service, using the `topic_arn` output.
* `alarm_sns_topic_arns_us_east_1`: The ARNs of SNS topics for receiving alerts from CloudWatch in `us-east-1` (route 53
health check alerts only report in the `us-east-1` region). This should be pulled in with a `dependency` block against
the `sns-topic-us-east-1` service, using the `topic_arn` output.
* `elb_target_group_vpc_id`: The ID of the VPC where the ELB target group should be created. This should be sourced
using a `dependency` block against the `vpc-app` service, using the `vpc_id` output.

=== Inputs for Backward Compatibility

Configure the following new inputs to ensure your service continues to function with minimal interruption. These are
necessary to maintain backward compatibility. _If left unset, you will risk redeploying the service and causing
downtime._

* `use_auto_scaling = false` (This now defaults to `true` in the Service Catalog version of the module).
* Set the following to avoid recreating the IAM roles, which in turn leads to the ECS service being recreated. This is
because the Service Catalog version of the module defaults to only using the `<SERVICE_NAME>` in the IAM role names.
** `custom_iam_role_name_prefix = "<SERVICE_NAME>-<ENVIRONMENT_NAME>"`
** `custom_task_execution_iam_role_name_prefix = "<SERVICE_NAME>-<ENVIRONMENT_NAME>"`
** `custom_ecs_service_role_name = "<SERVICE_NAME>-<ENVIRONMENT_NAME>"`
* If you are using `gruntkms` for your secrets management, set the following to ensure the ECS task IAM role retains the
policy to access the KMS key:
+
[source,python]
----
iam_policy = {
  KMSKeyAccess = {
    actions   = ["kms:Decrypt"]
    effect    = "Allow"
    resources = [dependency.kms_key.outputs.key_arn]
  }
}
----

=== Output Changes

Update downstream dependency references to use the new names of these outputs, which were renamed in the Service Catalog
version of the module.

* `ecs_service_arn` ⇒ `service_arn`
* NOTE: the keys for the following outputs correspond to the keys for the `elb_target_groups` input variable:
** `target_group_arn` ⇒ `target_group_arns`
** `target_group_name` ⇒ `target_group_names`

Remove references to the following outputs:

* `fully_qualified_domain_name`
* `metric_widget_target_group_host_count`
* `metric_widget_target_group_request_count`
* `metric_widget_target_group_connection_error_count`
* `metric_widget_target_group_response_time`
* `metric_widget_target_group_4xx_count`
* `metric_widget_target_group_5xx_count`

=== State Migration Script

Run the provided migration script (contents pasted below for convenience) to migrate the state in a backward compatible
way:

[source,python]
----
#!/bin/bash
# This script contains the state migration instructions for migrating ecs-service-with-alb to the Service Catalog from the old
# style Gruntwork Reference Architecture. Install this script and run it from the terragrunt live configuration
# directory of the module to perform the state operations.
#

set -e
set -o pipefail

# Import the helper functions from the repo root.
readonly infra_live_repo_root="$(git rev-parse --show-toplevel)"
source "$infra_live_repo_root/_scripts/migration_helpers.sh"

# The following routine identifies state address changes where the resource was renamed. These are migrated with state
# mv operations.
function move_operations {
  # Move a non-list resource to list resource
  # ECS Task Role
  local -r ecs_task_role_query='module.*aws_iam_role\.ecs_task$'
  local ecs_task_role_addr
  ecs_task_role_addr="$(find_state_address "$ecs_task_role_query")"
  if [[ "$ecs_task_role_addr" != "*[0]" ]]; then
    fuzzy_move_state "$ecs_task_role_query" 'aws_iam_role.tmp_addr' 'ECS Task Role (Temp)'
    fuzzy_move_state 'aws_iam_role.tmp_addr' "${ecs_task_role_addr}[0]" 'ECS Task Role'
  else
    log 'ECS Task Role already migrated.'
  fi

  # ECS Task Execution Role
  local -r ecs_task_exec_role_query='module.*aws_iam_role\.ecs_task_execution_role$'
  local ecs_task_exec_role_addr
  ecs_task_exec_role_addr="$(find_state_address "$ecs_task_exec_role_query")"
  if [[ "$ecs_task_exec_role_addr" != "*[0]" ]]; then
    fuzzy_move_state "$ecs_task_exec_role_query" 'aws_iam_role.tmp_addr' 'ECS Task Execution Role (Temp)'
    fuzzy_move_state 'aws_iam_role.tmp_addr' "${ecs_task_exec_role_addr}[0]" 'ECS Task Execution Role'
  else
    log 'ECS Task Execution Role already migrated.'
  fi
}

function run {
  assert_hcledit_is_installed
  assert_jq_is_installed

  log 'Identifying state move operations that are necessary'
  move_operations

  local -r new_listener_rule_addr_base='module.listener_rules.aws_lb_listener_rule.forward'
  # The "main-443" is from forward_rules input variable map key.
  local -r new_listener_rule_addr="${new_listener_rule_addr_base}[\"main-443\"]"
  fuzzy_import_move_state \
    'aws_alb_listener_rule.paths_to_route_to_this_service' \
    "$new_listener_rule_addr" \
    'resource.aws_alb_listener_rule.paths_to_route_to_this_service' \
    'listener rule'
}

run "$@"
----

=== Breaking Changes

* The following CloudWatch alarms are currently not supported by the Service Catalog version of the module, and will be
removed from Terraform state when you migrate:
** `module.target_group_cloudwatch_alarms.aws_cloudwatch_metric_alarm.tg_high_target_response_time`
** `module.target_group_cloudwatch_alarms.aws_cloudwatch_metric_alarm.tg_high_target_connection_error_count`
** `module.target_group_cloudwatch_alarms.aws_cloudwatch_metric_alarm.tg_high_request_count`
** `module.target_group_cloudwatch_alarms.aws_cloudwatch_metric_alarm.tg_high_http_code_target_5xx_count`
** `module.target_group_cloudwatch_alarms.aws_cloudwatch_metric_alarm.tg_high_http_code_target_4xx_count`
* *Cluster outage*. A number of IAM policies were reorganized in the module. This translates to a few recreations of IAM
policies (`destroy` + `create`). Since they apply at the policy level, these should not cause any service disruptions.
However, you may experience a brief (<1 minute) outage in AWS access from your services while the IAM policies are being
recreated.
