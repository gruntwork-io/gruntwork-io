---
title: ecs-cluster Service Migration Guide
categories: Upgrades
image: /assets/img/guides/refresh_icon.png
excerpt: Learn how to update your v1 Reference Architecture to use the Gruntwork Service Catalog.
tags: ["aws", "terraform", "terragrunt"]
cloud: ["aws"]
redirect_from: /static/guides/upgrades/how-to-update-your-ref-arch/
---
:page-type: guide
:page-layout: post

:toc:
:toc-placement!:

// GitHub specific settings. See https://gist.github.com/dcode/0cfbf2699a1fe9b46ff04c41721dda74 for details.
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
toc::[]
endif::[]

== ecs-cluster Service Migration Guide

Follow this guide to update the `ecs-cluster` module to the Service Catalog.

=== Estimated Time to Migrate: 30 minutes per environment

=== New AMI

Build a new AMI using the `packer` template in the Service Catalog
(`[modules/services/ecs-cluster/ecs-node-al2.json](https://github.com/gruntwork-io/terraform-aws-service-catalog/blob/master/modules/services/ecs-cluster/ecs-node-al2.json)`).

Follow
https://github.com/gruntwork-io/terraform-aws-service-catalog/blob/master/core-concepts.md#how-to-build-amis-for-the-service-catalog[the
instructions in the Service Catalog repository] to build a new AMI, and then set the `cluster_instance_ami` input
variable to the new AMI ID.

Note that this by itself will not rotate your existing ECS cluster workers, and is thus a backward compatible change.

=== New Required Inputs

Configure these new inputs to migrate to the Service Catalog version of the module. They are now required.

* `cluster_instance_ami_filters`: Set this to `null` . This provides an alternative mechanism to lookup the AMI to use
dynamically, but since you are providing the AMI ID directly, this variable needs to be turned off.
* `vpc_id`: Set this to the ID of the VPC where the ECS cluster workers should be deployed. This should be pulled in
using a `dependency` block against the `vpc-app` service, using the `vpc_id` output.
* `vpc_subnet_ids`: Set this to the list of VPC subnet IDs where the ECS cluster workers should be deployed. This should
be pulled in using a `dependency` block against the `vpc` live configuration, using the `private_app_subnet_ids` output.

Configure the following new inputs to ensure your service continues to function with minimal interruption:

* `public_alb_sg_ids`: Set this to the VPC Security Group ID of the public ALB. This should be pulled in using a
`dependency` block against the `alb-public` service, using the `alb_security_group_id` output.
* `internal_alb_sg_ids`: Set this to the VPC Security Group ID of the internal ALB. This should be pulled in using a
`dependency` block against the `alb-internal` service, using the `alb_security_group_id` output.
* `allow_ssh_from_security_group_ids`: Set this to the VPC Security Group ID of the network bastion server (either
OpenVPN server or Linux host) deployed in the `mgmt` VPC. This should be pulled in using a `dependency` block against
the `openvpn-server` (or `bastion-host`) service, using the `security_group_id` output.
* `alarms_sns_topic_arn`: Set this to the ARN of the SNS topic deployed in the region. This should be pulled in with a
`dependency` block against the `sns-topic` service, using the `topic_arn` output.

Configure the following input if you want to minimize changes. This is optional.

* `enable_autoscaling`: Set to `false` (this now defaults to `true` in the Service Catalog version of the module).

=== State Migration Script

Run the provided migration script (contents pasted below for convenience) to migrate the state in a backward compatible
way:

[source,python]
----
#!/bin/bash
# This script contains the state migration instructions for migrating ecs-cluster to the Service Catalog from the old
# style Gruntwork Reference Architecture. Install this script and run it from the terragrunt live configuration
# directory of the module to perform the state operations.
#

# Import the helper functions from the repo root.
readonly infra_live_repo_root="$(git rev-parse --show-toplevel)"
source "$infra_live_repo_root/_scripts/migration_helpers.sh"

function run {
  fuzzy_move_state \
    'module.ecs_cluster.aws_security_group_rule.allow_inbound_from_alb\[0\]' \
    'module.ecs_cluster.aws_security_group_rule.allow_inbound_from_alb[2]' \
    'Ingress security group rule from ALB (tmp)'
  fuzzy_move_state \
    'module.ecs_cluster.aws_security_group_rule.allow_inbound_from_alb\[1\]' \
    'module.ecs_cluster.aws_security_group_rule.allow_inbound_from_alb[0]' \
    'Ingress security group rule from ALB (swap)'
  fuzzy_move_state \
    'module.ecs_cluster.aws_security_group_rule.allow_inbound_from_alb\[2\]' \
    'module.ecs_cluster.aws_security_group_rule.allow_inbound_from_alb[1]' \
    'Ingress security group rule from ALB (swap)'
}

run "$@"
----

=== Breaking Changes

* *Cluster outage*. The IAM policies attached to the roles of the worker instances of the ECS cluster need to be
recreated due to a reorganization of how the policies are attached. This means that there will be a brief outage (< 1
minute) in log aggregation and metric reporting while the IAM policies are being recreated. This is unavoidable.
